import asyncio
from abc import ABC, abstractmethod
import os, time
from typing import Optional, Any

try: from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, GenerationConfig
except ImportError:
    raise ImportError(
        "Transformers library is required for HFLocalAgent. "
        "Install it with: pip install transformers"
    )

from textarena.core import Agent
import textarena as ta 


__all__ = [
    "HumanAgent",
    "OpenRouterAgent",
    "GeminiAgent",
    "OpenAIAgent",
    "HFLocalAgent",
    "CerebrasAgent",
    "AWSBedrockAgent",
    "AnthropicAgent",
]

DEFAULT_TEMPERATURE = 0.9

STANDARD_GAME_PROMPT = "You are a competitive game player. Make sure you read the game instructions carefully, and always follow the required format."
    
class HumanAgent(Agent):
    """ Human agent class that allows the user to input actions manually """
    def __init__(self):
        super().__init__()

    def __call__(self, observation: str) -> str:
        """
        Process the observation and return the action.
        
        Args:
            observation (str): The input string to process.
            
        Returns:
            str: The response generated by the agent.
        """
        return input(f"Current observations: {observation}\nPlease enter the action: ")


class OpenRouterAgent(Agent):
    """ Agent class using the OpenRouter API to generate responses. """
    def __init__(self, model_name: str, system_prompt: Optional[str] = STANDARD_GAME_PROMPT, verbose: bool = False, **kwargs):
        """
        Initialize the OpenRouter agent.

        Args:
            model_name (str): The name of the model.
            system_prompt (Optional[str]): The system prompt to use (default: STANDARD_GAME_PROMPT)
            verbose (bool): If True, additional debug info will be printed.
            **kwargs: Additional keyword arguments to pass to the OpenAI API call.
        """
        super().__init__()
        self.model_name = model_name 
        self.verbose = verbose 
        self.system_prompt = system_prompt
        self.kwargs = kwargs

        try:
            from openai import OpenAI
        except ImportError:
            raise ImportError(
                "OpenAI package is required for OpenRouterAgent. "
                "Install it with: pip install openai"
            )

        # Set the open router api key from an environment variable
        api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            raise ValueError("OpenRouter API key not found. Please set the OPENROUTER_API_KEY environment variable.")
        
        self.client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=api_key)
        # self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        

    def _make_request(self, observation: str) -> str:
        """ Make a single API request to OpenRouter and return the generated message. """
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": observation}
        ]
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            n=1,
            stop=None,
            **self.kwargs
        )
        return response.choices[0].message.content.strip()

    def _retry_request(self, observation: str, retries: int = 3, delay: int = 5) -> str:
        """
        Attempt to make an API request with retries.

        Args:
            observation (str): The input to process.
            retries (int): The number of attempts to try.
            delay (int): Seconds to wait between attempts.

        Raises:
            Exception: The last exception caught if all retries fail.
        """
        last_exception = None
        for attempt in range(1, retries + 1):
            try:
                response = self._make_request(observation)
                if self.verbose:
                    print(f"\nObservation: {observation}\nResponse: {response}")
                return response

            except Exception as e:
                last_exception = e
                print(f"Attempt {attempt} failed with error: {e}")
                if attempt < retries:
                    time.sleep(delay)
        raise last_exception

    def __call__(self, observation: str) -> str:
        """
        Process the observation using the OpenRouter API and return the action.

        Args:
            observation (str): The input string to process.

        Returns:
            str: The generated response.
        """
        if not isinstance(observation, str):
            raise ValueError(f"Observation must be a string. Received type: {type(observation)}")
        return self._retry_request(observation)


class GeminiAgent(Agent):
    """Agent class using the Google Gemini API to generate responses."""
    def __init__(
        self, 
        model_name: str, 
        system_prompt: Optional[str] = STANDARD_GAME_PROMPT,
        verbose: bool = False,
        generation_config: Optional[dict] = None
    ):
        """
        Initialize the Gemini agent.
        
        Args:
            model_name (str): The name of the model.
            system_prompt (Optional[str]): The system prompt to use (default: STANDARD_GAME_PROMPT).
            verbose (bool): If True, additional debug info will be printed.
            generation_config (Optional[dict]): The configuration for text generation.
        """
        super().__init__()
        self.model_name = model_name
        self.system_prompt = system_prompt
        self.verbose = verbose

        try:
            import google.generativeai as genai
        except ImportError:
            raise ImportError(
                "Google Generative AI package is required for GeminiAgent. "
                "Install it with: pip install google-generativeai"
            )
        
        # Set the Gemini API key from an environment variable
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("Gemini API key not found. Please set the GEMINI_API_KEY environment variable.")
        
        # Configure the Gemini client
        genai.configure(api_key=api_key)
        
        # Use default generation config if none is provided
        if generation_config is None:
            generation_config = {
                "temperature": 1,
                "top_p": 0.95,
                "top_k": 40,
                "max_output_tokens": 8192,
                "response_mime_type": "text/plain",
            }
        self.generation_config = generation_config
        
        # Create the Gemini model
        self.model = genai.GenerativeModel(
            model_name=self.model_name,
            generation_config=self.generation_config
        )
    
    def _make_request(self, observation: str) -> str:
        """
        Make a single API request to Gemini and return the generated message.
        
        Args:
            observation (str): The input string to process.
        
        Returns:
            str: The generated response text.
        """
        response = self.model.generate_content(
            f"Instructions: {self.system_prompt}\n\n{observation}"
        )
        
        if self.verbose:
            print(f"\nObservation: {observation}\nResponse: {response.text}")
        
        return response.text.strip()
    
    def _retry_request(self, observation: str, retries: int = 3, delay: int = 5) -> str:
        """
        Attempt to make an API request with retries.
        
        Args:
            observation (str): The input to process.
            retries (int): The number of attempts to try.
            delay (int): Seconds to wait between attempts.
        
        Raises:
            Exception: The last exception caught if all retries fail.
        """
        last_exception = None
        for attempt in range(1, retries + 1):
            try:
                return self._make_request(observation)
            except Exception as e:
                last_exception = e
                print(f"Attempt {attempt} failed with error: {e}")
                if attempt < retries:
                    time.sleep(delay)
        raise last_exception
    
    def __call__(self, observation: str) -> str:
        """
        Process the observation using the Gemini API and return the generated response.
        
        Args:
            observation (str): The input string to process.
        
        Returns:
            str: The generated response.
        """
        if not isinstance(observation, str):
            raise ValueError(f"Observation must be a string. Received type: {type(observation)}")
        return self._retry_request(observation)


class OpenAIAgent(Agent):
    """Agent class using the OpenAI API to generate responses."""
    
    def __init__(
        self, 
        model_name: str, 
        system_prompt: Optional[str] = STANDARD_GAME_PROMPT,
        verbose: bool = False,
        **kwargs
    ):
        """
        Initialize the OpenAI agent.
        
        Args:
            model_name (str): The name of the model.
            system_prompt (Optional[str]): The system prompt to use (default: STANDARD_GAME_PROMPT).
            verbose (bool): If True, additional debug info will be printed.
            **kwargs: Additional keyword arguments to pass to the OpenAI API call.
        """
        super().__init__()
        self.model_name = model_name
        self.system_prompt = system_prompt
        self.verbose = verbose
        self.kwargs = kwargs

        try:
            from openai import OpenAI
        except ImportError:
            raise ImportError(
                "OpenAI package is required for OpenAIAgent. "
                "Install it with: pip install openai"
            )

        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.")
        
        self.client = OpenAI(api_key=api_key)
        
    
    def _make_request(self, observation: str) -> str:
        """
        Make a single API request to OpenAI and return the generated message.
        
        Args:
            observation (str): The input string to process.
        
        Returns:
            str: The generated response text.
        """
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": observation}
        ]
        
        # Make the API call using the provided model and messages.
        completion = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            n=1,
            stop=None,
            **self.kwargs
        )
        
        return completion.choices[0].message.content.strip()
    
    def _retry_request(self, observation: str, retries: int = 3, delay: int = 5) -> str:
        """
        Attempt to make an API request with retries.
        
        Args:
            observation (str): The input to process.
            retries (int): The number of attempts to try.
            delay (int): Seconds to wait between attempts.
        
        Raises:
            Exception: The last exception caught if all retries fail.
        """
        last_exception = None
        for attempt in range(1, retries + 1):
            try:
                response = self._make_request(observation)
                if self.verbose:
                    print(f"\nObservation: {observation}\nResponse: {response}")
                return response
            except Exception as e:
                last_exception = e
                print(f"Attempt {attempt} failed with error: {e}")
                if attempt < retries:
                    time.sleep(delay)
        raise last_exception
    
    def __call__(self, observation: str) -> str:
        """
        Process the observation using the OpenAI API and return the generated response.
        
        Args:
            observation (str): The input string to process.
        
        Returns:
            str: The generated response.
        """
        if not isinstance(observation, str):
            raise ValueError(f"Observation must be a string. Received type: {type(observation)}")
        return self._retry_request(observation)


class HFLocalAgent(Agent):
    """ Hugging Face local agent class that uses the Hugging Face Transformers library """
    def __init__(
        self, 
        model_name: Optional[str] = None,
        model: Optional[Any] = None,
        tokenizer: Optional[Any] = None,
        device: str = "auto", 
        quantize: bool = False, 
        max_new_tokens: int = 1024,
        sample: bool = True,
        temperature: float = DEFAULT_TEMPERATURE,
    ):
        """
        Initialize the Hugging Face local agent.
        
        Args:
            model_name (Optional[str]): The name of the model to load from Hugging Face.
            model (Optional[Any]): A pre-loaded Hugging Face model.
            tokenizer (Optional[Any]): A pre-loaded Hugging Face tokenizer.
            device (str): Device to use for model inference (default: "auto").
            quantize (bool): Whether to load the model in 8-bit quantized format (default: False).
            max_new_tokens (int): Maximum number of new tokens to generate.
            
        Raises:
            ValueError: If both model_name and model are provided, or if neither is provided.
        """
        super().__init__()
        
        if model_name is not None and model is not None:
            raise ValueError("Cannot provide both model_name and model. Please provide only one.")
        if model_name is None and model is None:
            raise ValueError("Must provide either model_name or model.")
            
            
        self.device = device
        self.quantize = quantize
        self.max_new_tokens = max_new_tokens
        self.system_prompt = STANDARD_GAME_PROMPT
        
        # Initialize model and tokenizer
        if model_name is not None:
            self._load_model_from_name(model_name)
        else:
            self.model = model
            self.tokenizer = tokenizer or AutoTokenizer.from_pretrained(model.config._name_or_path)
            

        # Initialize the generation config
        if sample:
            self.generation_config = GenerationConfig(
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=True,
                bos_token_id=self.tokenizer.bos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id,
            )
        else:
            self.generation_config = GenerationConfig(
                max_new_tokens=max_new_tokens, 
                do_sample=False,
                bos_token_id=self.tokenizer.bos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        # Initialize the pipeline
        self._init_pipeline()
    
    def _load_model_from_name(self, model_name: str):
        """Load model and tokenizer from model name."""
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        if self.quantize:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name, 
                load_in_8bit=True,
                device_map=self.device,
            )
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map=self.device,
            )
    
    def _init_pipeline(self):
        """Initialize the text generation pipeline."""
        from transformers import pipeline
        
        self.pipeline = pipeline(
            'text-generation',
            max_new_tokens=self.max_new_tokens,
            model=self.model, 
            tokenizer=self.tokenizer, 
            generation_config=self.generation_config
        )
    
    def update_model(
        self, 
        new_model: Optional[Any] = None, 
        new_model_name: Optional[str] = None,
        new_tokenizer: Optional[Any] = None,
        clear_gpu: bool = True
    ):
        """
        Update the model and/or tokenizer with new ones.
        
        Args:
            new_model (Optional[Any]): New model to use.
            new_model_name (Optional[str]): New model name to load.
            new_tokenizer (Optional[Any]): New tokenizer to use.
            clear_gpu (bool): Whether to clear GPU memory before loading new model.
            
        Raises:
            ValueError: If both new_model and new_model_name are provided.
        """
        if new_model is not None and new_model_name is not None:
            raise ValueError("Cannot provide both new_model and new_model_name. Please provide only one.")
            
        if clear_gpu:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                del self.model
                del self.tokenizer
                del self.pipeline
        
        if new_model_name is not None:
            self._load_model_from_name(new_model_name)
        elif new_model is not None:
            self.model = new_model
            self.tokenizer = new_tokenizer or self.tokenizer
            
        self._init_pipeline()
    
    def __call__(self, observation: str) -> str:
        """
        Process the observation using the Hugging Face model and return the action.
        
        Args:
            observation (str): The input string to process.
        
        Returns:
            str: The response generated by the model.
        """
        # Generate a response
        try:
            response = self.pipeline(
                self.system_prompt+"\n"+observation, 
                num_return_sequences=1, 
                return_full_text=False,
                generate_kwargs={"generation_config": self.generation_config}
            )
            # Extract and return the text output
            action = response[0]['generated_text'].strip()
            return action
        except Exception as e:
            return f"An error occurred: {e}"



class CerebrasAgent(Agent):
    """ Cerebras agent class that uses the Cerebras API to generate responses """
    def __init__(self, model_name: str, system_prompt: str | None = None):
        """
        Initialize the Cerebras agent.

        Args:
            model_name (str): The name of the model.
            system_prompt (str): The system prompt to use (default: "You are a competitive game player.").
        """
        super().__init__()
        self.model_name = model_name
        
        try:
            from cerebras.cloud.sdk import Cerebras
        except ImportError:
            raise ImportError(
                "Cerebras SDK is required for CerebrasAgent. "
                "Install it with: pip install cerebras-cloud-sdk"
            )
            
        self.client = Cerebras(
            # This is the default and can be omitted
            api_key=os.getenv("CEREBRAS_API_KEY"),
        )

        ## Set the system prompt
        if system_prompt is None:
            self.system_prompt = "You are a competitive game player. Make sure you read the game instructions carefully, and always follow the required format."
        else:
            self.system_prompt = system_prompt

    def __call__(self, observation: str) -> str:
        """
        Process the observation using the Cerebras model and return the action.

        Args:
            observation (str): The input string to process.

        Returns:
            str: The response generated by the model.
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": observation},
                ],
                top_p=0.9,
                temperature=0.9,
            )
            # Extract the assistant's reply
            action = response.choices[0].message.content.strip()
            return action
        except Exception as e:
            return f"An error occurred: {e}"


class AWSBedrockAgent(Agent):
    """
    AWS Bedrock agent class that interacts with Claude 3 Haiku via AWS Bedrock Runtime API.
    """
    def __init__(
        self, 
        model_id: str, 
        region_name: str = "us-east-1", 
        system_prompt: Optional[str] = STANDARD_GAME_PROMPT,
        verbose: bool = False,
        **kwargs
    ):
        """
        Initialize the AWS Bedrock agent.
        
        Args:
            model_name (str): The ID of the AWS Bedrock model to use.
            region_name (str): AWS region for Bedrock service.
            system_prompt (Optional[str]): The system prompt to use.
            verbose (bool): If True, print debug information.
            **kwargs: Additional parameters for inference configuration.
        """
        super().__init__()
        self.model_id = model_id
        self.region_name = region_name
        self.system_prompt = system_prompt
        self.verbose = verbose
        self.kwargs = kwargs

        try:
            import boto3
            from botocore.exceptions import ClientError
        except ImportError:
            raise ImportError(
                "Boto3 is required for AWSBedrockAgent. "
                "Install it with: pip install boto3"
            )

        self.client = boto3.client("bedrock-runtime", region_name=self.region_name)

    def _make_request(self, observation: str) -> str:
        """ Make an API request to AWS Bedrock."""
        conversation = [
            {"role": "user", "content": [{"text": observation}]}
        ]

        systemPrompt = [{ "text": self.system_prompt }]
        
        try:
            response = self.client.converse(
                modelId=self.model_id,
                messages=conversation,
                system=systemPrompt,
                inferenceConfig={"maxTokens": 512, "temperature": 0.9, "topP": 0.9, **self.kwargs},
            )
            response_text = response["output"]["message"]["content"][0]["text"].strip()
            if self.verbose:
                print(f"\nObservation: {observation}\nResponse: {response_text}")
            return response_text
        except Exception as e:
            return f"ERROR: Can't invoke '{self.model_id}'. Reason: {e}"

    def __call__(self, observation: str) -> str:
        """ Process the observation using AWS Bedrock and return the response."""
        if not isinstance(observation, str):
            raise ValueError(f"Observation must be a string. Received type: {type(observation)}")
        return self._make_request(observation)


class AnthropicAgent(Agent):
    """Agent class using the Anthropic Claude API to generate responses."""
    def __init__(self, model_name: str, system_prompt: Optional[str] = STANDARD_GAME_PROMPT, max_tokens: int = 1000, temperature: float = 0.9, verbose: bool = False):
        """
        Initialize the Anthropic agent.

        Args:
            model_name (str): The name of the Claude model (e.g., "claude-3-5-sonnet-20241022").
            system_prompt (Optional[str]): The system prompt to use (default: STANDARD_GAME_PROMPT).
            max_tokens (int): The maximum number of tokens to generate.
            temperature (float): The temperature for randomness in response generation.
            verbose (bool): If True, additional debug info will be printed.
        """
        super().__init__()
        self.model_name = model_name
        self.system_prompt = system_prompt
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.verbose = verbose
        
        try:
            import anthropic
        except ImportError:
            raise ImportError(
                "Anthropic package is required for AnthropicAgent. "
                "Install it with: pip install anthropic"
            )
            
        self.client = anthropic.Anthropic()
    
    def _make_request(self, observation: str) -> str:
        """Make a single API request to Anthropic and return the generated message."""
        response = self.client.messages.create(
            model=self.model_name,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            system=self.system_prompt,
            messages=[
                {"role": "user", "content": [{"type": "text", "text": observation}]}
            ]
        )
        
        return response.content[0].text.strip()
    
    def _retry_request(self, observation: str, retries: int = 3, delay: int = 5) -> str:
        """
        Attempt to make an API request with retries.

        Args:
            observation (str): The input to process.
            retries (int): The number of attempts to try.
            delay (int): Seconds to wait between attempts.

        Raises:
            Exception: The last exception caught if all retries fail.
        """
        last_exception = None
        for attempt in range(1, retries + 1):
            try:
                response = self._make_request(observation)
                if self.verbose:
                    print(f"\nObservation: {observation}\nResponse: {response}")
                return response
            except Exception as e:
                last_exception = e
                print(f"Attempt {attempt} failed with error: {e}")
                if attempt < retries:
                    time.sleep(delay)
        raise last_exception
    
    def __call__(self, observation: str) -> str:
        """
        Process the observation using the Anthropic API and return the generated response.
        
        Args:
            observation (str): The input string to process.
        
        Returns:
            str: The generated response.
        """
        if not isinstance(observation, str):
            raise ValueError(f"Observation must be a string. Received type: {type(observation)}")
        return self._retry_request(observation)

class AsyncAnthropicAgent(Agent):
    """Agent class using the Anthropic Claude API to generate responses asynchronously."""
    def __init__(self, model_name: str, system_prompt: Optional[str] = STANDARD_GAME_PROMPT, max_tokens: int = 1000, temperature: float = 0.9, verbose: bool = False):
        """
        Initialize the Anthropic agent.

        Args:
            model_name (str): The name of the Claude model (e.g., "claude-3-5-sonnet-20241022").
            system_prompt (Optional[str]): The system prompt to use (default: STANDARD_GAME_PROMPT).
            max_tokens (int): The maximum number of tokens to generate.
            temperature (float): The temperature for randomness in response generation.
            verbose (bool): If True, additional debug info will be printed.
        """
        super().__init__()
        self.model_name = model_name
        self.system_prompt = system_prompt
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.verbose = verbose
        
        try:
            import anthropic
        except ImportError:
            raise ImportError(
                "Anthropic package is required for AsyncAnthropicAgent. "
                "Install it with: pip install anthropic"
            )
            
        self.client = anthropic.AsyncAnthropic()
    
    async def _make_request(self, observation: str) -> str:
        """Make a single API request to Anthropic and return the generated message."""
        response = await self.client.messages.create(
            model=self.model_name,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            system=self.system_prompt,
            messages=[
                {"role": "user", "content": [{"type": "text", "text": observation}]}
            ]
        )
        
        return response.content[0].text.strip()
    
    async def _retry_request(self, observation: str, retries: int = 3, delay: int = 5) -> str:
        """
        Attempt to make an API request with retries.

        Args:
            observation (str): The input to process.
            retries (int): The number of attempts to try.
            delay (int): Seconds to wait between attempts.

        Raises:
            Exception: The last exception caught if all retries fail.
        """
        last_exception = None
        for attempt in range(1, retries + 1):
            try:
                response = await self._make_request(observation)
                if self.verbose:
                    print(f"\nObservation: {observation}\nResponse: {response}")
                return response
            except Exception as e:
                last_exception = e
                print(f"Attempt {attempt} failed with error: {e}")
                if attempt < retries:
                    await asyncio.sleep(delay)
        raise last_exception
    
    async def __call__(self, observation: str) -> str:
        """
        Process the observation using the Anthropic API and return the generated response.
        
        Args:
            observation (str): The input string to process.
        
        Returns:
            str: The generated response.
        """
        if not isinstance(observation, str):
            raise ValueError(f"Observation must be a string. Received type: {type(observation)}")
        return await self._retry_request(observation)